{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1cae999",
   "metadata": {},
   "source": [
    "# PharmAI Demo: Building an LLM Powered Pharmacometric Workflow\n",
    "This notebook is the practical, hands-on guide accompanying the blog post '[The Foundation: LLM Powered Pharmacometric Workflows](https://arianthony.github.io/demo/2025/06/09/from-task-to-code-building-reliable-llm-workflows-with-langchain.html)'. It turns the vision of an AI-powered pharmacometric workflow into executable code.\n",
    "\n",
    "We will build the foundational 'atom' of an AI agent, which operates on a **Plan, Act, Observe, Reflect** cycle:\n",
    "1.  **Plan:** The LLM analyzes a task and creates an R script—its plan of action.\n",
    "2.  **Act:** We execute that script in a controlled, safe environment.\n",
    "3.  **Observe:** We capture the results of the action—the `STDOUT` and `STDERR`.\n",
    "4.  **Reflect:** We feed these observations back to the LLM to evaluate the outcome and generate a corrected plan if needed.\n",
    "\n",
    "This iterative feedback loop is what transforms a simple script into a basic AI agent capable of self-correction. We achieve this using:\n",
    "- **LangChain:** For orchestrating the agentic loop.\n",
    "- **Local LLMs via Ollama:** For secure, on-premises execution, ensuring no data leaves your environment.\n",
    "- **Pydantic:** For reliable, structured output, making our agent's actions auditable and reproducible.\n",
    "- **Safe R Execution:** For the crucial 'Act' and 'Observe' steps of the cycle.\n",
    "\n",
    "This approach is perfect for pharmaceutical environments requiring security, reliability, and reproducibility. \n",
    "\n",
    "**On Data Security:** In this demo I am using Ollama to run local LLMs, ensuring that all data remains within your environment. However, foundational model providers like [Anthropic](https://www.anthropic.com/), [OpenAI](https://openai.com/), and [AWS Bedrock](https://aws.amazon.com/bedrock/) offer enterprise-grade security. Using these models will greatly enhance the capabilities of this agentic workflow, and thanks to LangChain's modular design, you can easily swap in these providers without changing the core logic of the agent (e.g., ChatBedrock instead of ChatOllama).\n",
    "\n",
    "**Windows Ollama Installation:**\n",
    "\n",
    "1. Download the Windows installer from [https://ollama.com/download](https://ollama.com/download)\n",
    "2. Run the installer and follow the prompts\n",
    "3. After installation, open a new terminal and run:\n",
    "   ```powershell\n",
    "   ollama run qwen2.5-coder:7b\n",
    "   ```\n",
    "   (This will download and start the model)\n",
    "4. For more details, see the [Ollama Windows Guide](https://github.com/ollama/ollama/blob/main/docs/windows.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be2eda24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama is running\n",
      "Available models: qwen2.5-coder:7b                       dae161e27b0e    4.7 GB    3 hours ago      \n"
     ]
    }
   ],
   "source": [
    "# Install requirements if needed\n",
    "# !pip install langchain langchain-community pydantic ollama pandas\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Check Ollama is running\n",
    "try:\n",
    "    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "    print(\"✅ Ollama is running\")\n",
    "    print(\"Available models:\", result.stdout.split('\\n')[1] if result.stdout else \"No models found\")\n",
    "except:\n",
    "    print(\"❌ Ollama not found. Install with: curl -fsSL https://ollama.com/install.sh | sh\")\n",
    "    print(\"Then: ollama pull qwen2.5:7b\")\n",
    "\n",
    "LLM_MODEL = \"qwen2.5-coder:7b\" # High performing model with tool use capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9384000",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 1: Define Our Data Structures\n",
    "\n",
    "Using Pydantic for consistent structured output from our LLM calls. This is essential for ensuring the LLM-generated code adheres to our expected format and can be safely executed within our workflow. See the LangChain structured output documentation for more details: [LangChain Structured Output](https://python.langchain.com/docs/concepts/structured_outputs/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4946b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input schema \n",
    "class TaskInput(BaseModel):\n",
    "    \"\"\"Structure for pharmacometric analysis tasks\"\"\"\n",
    "    task_name: str = Field(description=\"Short name for the analysis\")\n",
    "    task_details: str = Field(description=\"Natural language description of what to do\")\n",
    "    data_directory: str = Field(default=\"./data\", description=\"Path to data files\")\n",
    "\n",
    "# Define our output schema\n",
    "class AnalysisOutput(BaseModel):\n",
    "    \"\"\"What our LLM returns\"\"\"\n",
    "    script: str = Field(description=\"Complete executable R script\")\n",
    "    thoughts: str = Field(description=\"LLM reasoning about the approach\")\n",
    "    status_complete: bool = Field(default=False, description=\"Whether the task is complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be79b2",
   "metadata": {},
   "source": [
    "## Step 2: Build up the context for the LLM\n",
    "The critical step for successful LLM interaction is providing it with the right context. This should include all the information we would need to do the analysis. So we will start by providing a preview of our data (as a string). Then we will give it an example of the task we want to perform (nlmixr2 in this case). Then we will format the prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69d53bd",
   "metadata": {},
   "source": [
    "### Putting data into context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fb7ff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data context loaded:\n",
      " \n",
      "--- pd.head() for file: simulated_pk_data.csv ---\n",
      "   ID  TIME        DV  AMT  EVID\n",
      "0   1   0.0  0.010000  100     1\n",
      "1   1   0.5  0.971317    0     0\n",
      "2   1   1.0  1.394780    0     0\n",
      "3   1   2.0  2.022982    0     0\n",
      "4   1   4.0  1.545126    0     0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generalized data preview for all CSVs in DATA_DIR\n",
    "data_context = \"\"\n",
    "DATA_DIR = \"./data\"\n",
    "for csv_file in Path(DATA_DIR).glob(\"*.csv\"):\n",
    "    try:\n",
    "        df_head = pd.read_csv(csv_file).head().to_string()\n",
    "        data_context += f\"\\n--- pd.head() for file: {csv_file.name} ---\\n{df_head}\\n\"\n",
    "    except Exception as e:\n",
    "        data_context += f\"\\n--- {csv_file.name} ---\\nError reading file: {e}\\n\"\n",
    "print(\"Data context loaded:\\n\", data_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9509fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for nlmixr2 documentation\n",
    "example_context = \"\"\"\n",
    "library(nlmixr2)\n",
    "\n",
    "## The basic model consists of an ini block that has initial estimates\n",
    "one.compartment <- function() {\n",
    "  ini({\n",
    "    tka <- log(1.57); label(\"Ka\")\n",
    "    tcl <- log(2.72); label(\"Cl\")\n",
    "    tv <- log(31.5); label(\"V\")\n",
    "    eta.ka ~ 0.6\n",
    "    eta.cl ~ 0.3\n",
    "    eta.v ~ 0.1\n",
    "    add.sd <- 0.7\n",
    "  })\n",
    "  # and a model block with the error specification and model specification\n",
    "  model({\n",
    "    ka <- exp(tka + eta.ka)\n",
    "    cl <- exp(tcl + eta.cl)\n",
    "    v <- exp(tv + eta.v)\n",
    "    d/dt(depot) <- -ka * depot\n",
    "    d/dt(center) <- ka * depot - cl / v * center\n",
    "    cp <- center / v\n",
    "    cp ~ add(add.sd)\n",
    "  })\n",
    "}\n",
    "\n",
    "## The fit is performed by the function nlmixr/nlmixr2 specifying the model, data and estimate\n",
    "fit <- nlmixr(one.compartment, theo_sd, \"saem\",\n",
    "              control=list(print=0), \n",
    "              table=list(cwres=TRUE, npde=TRUE))\n",
    "\n",
    "# Print summary of parameter estimates and confidence intervals\n",
    "print(fit)\n",
    "\n",
    "# Basic Goodness of Fit Plots\n",
    "plot(fit)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a7e56",
   "metadata": {},
   "source": [
    "## Step 2: Build the Initial LLM Chain\n",
    "\n",
    "This is where the magic happens - converting natural language to R code. I will first demonstrate this in a linear \"toy\" example so you can get a feel for how LLMs ingest context and produce output. Technically this is a chain and might not be called agentic yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b7dcbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Define a sample analysis task\n",
    "sample_task = TaskInput(\n",
    "    task_name=\"fit_poppk_model\",\n",
    "    task_details=\"\"\"\n",
    "    Load population pharmacokinetic data from the data directory.\n",
    "    Fit a nonlinear mixed effects model using nlmixr2. \n",
    "    Print a summary table of parameter estimates and confidence intervals.\n",
    "    Use ggplot2 for visualization.\n",
    "    Write complete, executable R scripts using tidyverse principles\n",
    "    \"\"\",\n",
    "    data_directory=\"./data\"\n",
    ")\n",
    "\n",
    "# Define the system prompt \n",
    "system_prompt = f\"\"\"You are an expert pharmacometrician generating R scripts. Data for your analysis is located in {sample_task.data_directory}.\n",
    "\n",
    "Here is a summary of the data:\n",
    "<data_context>\n",
    "{data_context}\n",
    "</data_context>\n",
    "\n",
    "<task> \n",
    "{sample_task.task_name} \n",
    "</task> \n",
    "\n",
    "<task_details>\n",
    "{sample_task.task_details}\n",
    "</task_details>\n",
    "\n",
    "<example_context>\n",
    "{example_context}\n",
    "</example_context>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced46125",
   "metadata": {},
   "source": [
    "## Step 3: Define the LLM Chain\n",
    "Now we will define the LLM chain that will take our task description and convert it into executable R code. This is where we leverage LangChain's structured output capabilities to ensure the generated code is safe and adheres to our expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72f517b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Generating analysis script...\n",
      "✅ Analysis script generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Build the messages list\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=\"Please generate the R script for the above task.\")\n",
    "]\n",
    "\n",
    "# Set up the LLM and parser\n",
    "llm = ChatOllama(\n",
    "    model=LLM_MODEL,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Use structured output with Pydantic\n",
    "llm_struct = llm.with_structured_output(AnalysisOutput)\n",
    "\n",
    "# Call the LLM (this may take up to a few minutes depending on the task complexity)\n",
    "print(\"🧠 Generating analysis script...\")\n",
    "generated_analysis = llm_struct.invoke(messages)\n",
    "print(\"✅ Analysis script generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8387097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Analysis generated!\n",
      "Completed Status: False\n",
      "\n",
      "LLM thoughts: The script loads the population pharmacokinetic data from a CSV file. It then defines a one-compartment model with inter-individual variability using nlmixr2 syntax. The model is fitted using the SAEM algorithm, and the results are printed along with basic goodness-of-fit plots.\n",
      "\n",
      "📝 Generated R script (922 characters):\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ Analysis generated!\")\n",
    "print(f\"Completed Status: {generated_analysis.status_complete}\")\n",
    "print(f\"\\nLLM thoughts: {generated_analysis.thoughts}\")\n",
    "print(f\"\\n📝 Generated R script ({len(generated_analysis.script)} characters):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d651a99",
   "metadata": {},
   "source": [
    "## Step 4: R Script Execution\n",
    "\n",
    "Now that we've done a single pass we can write the script to a file and execute it. This is where we can use the `subprocess` module to run the R script in a controlled environment.\n",
    "We will also capture the output and any errors to ensure we can debug if something goes wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5d0892c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write R script to the current directory\n",
    "script_path = Path(sample_task.task_name.replace(\" \", \"_\") + \"_analysis.R\")\n",
    "script_path.write_text(generated_analysis.script)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec4d27",
   "metadata": {},
   "source": [
    "### Check out the Script\n",
    "This is a good place to stop and look at the generated R script. This is the code that will be executed in the next step. You can review it and get a feel for how the LLM has interpreted the task and generated the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff906e",
   "metadata": {},
   "source": [
    "## Step 5: Execute the R Script\n",
    "\n",
    "Finally, we will execute the generated R script using the `subprocess` module. This allows us to run the code in a controlled environment and capture any output or errors for debugging purposes. Note that this is not a deterministic process, so the output may vary depending on the LLM's interpretation of the task and the data provided. During my testing, this ran successfully about 50% of the time. If you're testing fails don't worry! This is just a simple linear chain and we have not added the iterative agentic capabilities yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dce9f951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT:\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "[====|====|====|====|====|====|====|====|====|====] 0:00:00 \n",
      "\n",
      "â”€â”€ nlmixrÂ² SAEM OBJF by FOCEi approximation â”€â”€\n",
      "\n",
      "          OBJF       AIC       BIC Log-likelihood Condition#(Cov)\n",
      "FOCEi -382.577 -37.75912 -15.40843       25.87956        2.173765\n",
      "      Condition#(Cor)\n",
      "FOCEi        1.808663\n",
      "\n",
      "â”€â”€ Time (sec fit$time): â”€â”€\n",
      "\n",
      "        setup optimize covariance  saem table compress\n",
      "elapsed 0.002        0          0 15.44  9.39     0.03\n",
      "\n",
      "â”€â”€ Population Parameters (fit$parFixed or fit$parFixedDf): â”€â”€\n",
      "\n",
      "       Parameter  Est.     SE %RSE Back-transformed(95%CI) BSV(CV%) Shrink(SD)%\n",
      "tka           Ka  0.12  0.072 59.8        1.13 (0.98, 1.3)     19.6      39.2% \n",
      "tcl           Cl  1.58 0.0773  4.9       4.85 (4.16, 5.64)     32.8      4.72% \n",
      "tv             V  3.94 0.0609 1.55         51.5 (45.7, 58)     25.0      4.05% \n",
      "add.sd           0.156                               0.156                     \n",
      " \n",
      "  Covariance Type (fit$covMethod): linFim\n",
      "  No correlations in between subject variability (BSV) matrix\n",
      "  Full BSV covariance (fit$omega) or correlation (fit$omegaR; diagonals=SDs) \n",
      "  Distribution stats (mean/skewness/kurtosis/p-value) available in fit$shrink \n",
      "  Censoring (fit$censInformation): No censoring\n",
      "\n",
      "â”€â”€ Fit Data (object fit is a modified tibble): â”€â”€\n",
      "# A tibble: 180 Ã— 28\n",
      "  ID     TIME    DV EPRED  ERES  NPDE   NPD    PDE    PD  PRED   RES  WRES IPRED\n",
      "  <fct> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n",
      "1 1       0.5 0.971 0.864 0.108 -1.01 0.412 0.157  0.66  0.817 0.155 0.559  1.01\n",
      "2 1       1   1.39  1.26  0.132 -1.68 0.431 0.0467 0.667 1.24  0.151 0.427  1.51\n",
      "3 1       2   2.02  1.53  0.493  1.41 1.30  0.92   0.903 1.53  0.489 1.29   1.80\n",
      "# â„¹ 177 more rows\n",
      "# â„¹ 15 more variables: IRES <dbl>, IWRES <dbl>, CPRED <dbl>, CRES <dbl>,\n",
      "#   CWRES <dbl>, eta.ka <dbl>, eta.cl <dbl>, eta.v <dbl>, depot <dbl>,\n",
      "#   center <dbl>, ka <dbl>, cl <dbl>, v <dbl>, tad <dbl>, dosenum <dbl>\n",
      "\n",
      "\n",
      "STDERR:\n",
      "Loading required package: nlmixr2data\n",
      "â„¹ parameter labels from comments are typically ignored in non-interactive mode\n",
      "â„¹ Need to run with the source intact to parse comments\n",
      "â†’ loading into symengine environment...\n",
      "â†’ pruning branches (`if`/`else`) of saem model...\n",
      "âœ” done\n",
      "â†’ finding duplicate expressions in saem model...\n",
      "â†’ optimizing duplicate expressions in saem model...\n",
      "âœ” done\n",
      "using C compiler: 'gcc.exe (GCC) 14.2.0'\n",
      "\n",
      "\n",
      "â„¹ calculate uninformed etas\n",
      "â„¹ done\n",
      "rxode2 3.0.4 using 2 threads (see ?getRxThreads)\n",
      "  no cache: create with `rxCreateCache()`\n",
      "Calculating covariance matrix\n",
      "â†’ loading into symengine environment...\n",
      "â†’ pruning branches (`if`/`else`) of saem model...\n",
      "âœ” done\n",
      "â†’ finding duplicate expressions in saem predOnly model 0...\n",
      "â†’ finding duplicate expressions in saem predOnly model 1...\n",
      "â†’ optimizing duplicate expressions in saem predOnly model 1...\n",
      "â†’ finding duplicate expressions in saem predOnly model 2...\n",
      "âœ” done\n",
      "using C compiler: 'gcc.exe (GCC) 14.2.0'\n",
      "\n",
      "\n",
      "â†’ Calculating residuals/tables\n",
      "â†’ loading into symengine environment...\n",
      "â†’ pruning branches (`if`/`else`) of full model...\n",
      "âœ” done\n",
      "â†’ calculate jacobian\n",
      "â†’ calculate sensitivities\n",
      "â†’ calculate âˆ‚(f)/âˆ‚(Î·)\n",
      "â†’ calculate âˆ‚(RÂ²)/âˆ‚(Î·)\n",
      "â†’ finding duplicate expressions in inner model...\n",
      "â†’ optimizing duplicate expressions in inner model...\n",
      "â†’ finding duplicate expressions in EBE model...\n",
      "â†’ optimizing duplicate expressions in EBE model...\n",
      "â†’ compiling inner model...\n",
      "using C compiler: 'gcc.exe (GCC) 14.2.0'\n",
      "\n",
      "\n",
      "âœ” done\n",
      "â†’ finding duplicate expressions in FD model...\n",
      "â†’ optimizing duplicate expressions in FD model...\n",
      "â†’ compiling EBE model...\n",
      "using C compiler: 'gcc.exe (GCC) 14.2.0'\n",
      "\n",
      "\n",
      "âœ” done\n",
      "â†’ compiling events FD model...\n",
      "using C compiler: 'gcc.exe (GCC) 14.2.0'\n",
      "\n",
      "\n",
      "âœ” done\n",
      "using C compiler: 'gcc.exe (GCC) 14.2.0'\n",
      "\n",
      "\n",
      "âœ” done\n",
      "â†’ compress origData in nlmixr2 object, save 5144\n",
      "â†’ compress phiM in nlmixr2 object, save 111008\n",
      "â†’ compress parHistData in nlmixr2 object, save 14040\n",
      "â†’ compress saem0 in nlmixr2 object, save 34496\n",
      "â†’ loading into symengine environment...\n",
      "â†’ pruning branches (`if`/`else`) of full model...\n",
      "âœ” done\n",
      "â†’ calculate jacobian\n",
      "â†’ calculate sensitivities\n",
      "â†’ calculate âˆ‚(f)/âˆ‚(Î·)\n",
      "â†’ calculate âˆ‚(RÂ²)/âˆ‚(Î·)\n",
      "â†’ finding duplicate expressions in inner model...\n",
      "â†’ optimizing duplicate expressions in inner model...\n",
      "â†’ finding duplicate expressions in EBE model...\n",
      "â†’ optimizing duplicate expressions in EBE model...\n",
      "â†’ compiling inner model...\n",
      "âœ” done\n",
      "â†’ finding duplicate expressions in FD model...\n",
      "â†’ optimizing duplicate expressions in FD model...\n",
      "â†’ compiling EBE model...\n",
      "âœ” done\n",
      "â†’ compiling events FD model...\n",
      "âœ” done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute R script in the current directory\n",
    "result = subprocess.run(\n",
    "    [\"Rscript\", str(script_path)],\n",
    "    cwd=\".\",\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    timeout=180\n",
    ")\n",
    "\n",
    "output = f\"STDOUT:\\n{result.stdout}\\n\\nSTDERR:\\n{result.stderr}\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da5064",
   "metadata": {},
   "source": [
    "## Step 6: Giving the Model Some Feedback\n",
    "Now that we have executed the R script, we can provide feedback to the model. This is an important step in the iterative process of refining the LLM's output. We will capture the output and any errors from the R script execution and use this information to improve the model's understanding of the task. Even if the script was executed sucessfully, we can still provide feedback to help the model learn and improve based on the output of the R script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25a7f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the model output and the script results to the messages\n",
    "new_messages = messages.copy()\n",
    "new_messages.append(\n",
    "    AIMessage(content=generated_analysis.model_dump_json())\n",
    ")\n",
    "\n",
    "new_messages.append(\n",
    "    HumanMessage(content=output)\n",
    ")\n",
    "\n",
    "# Pass the updated messages back to the LLM to try again\n",
    "new_generated_analysis = llm_struct.invoke(new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf88df87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LLM thoughts after execution:\n",
      "The script loads the population pharmacokinetic data from a CSV file. It then defines a one-compartment model with inter-individual variability using nlmixr2 syntax. The model is fitted using the SAEM algorithm, and the results are printed along with basic goodness-of-fit plots.\n",
      "✅ Final status complete: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at what how the model reacted by printing the thoughts\n",
    "print(\"🧠 LLM thoughts after execution:\"\n",
    "      f\"\\n{new_generated_analysis.thoughts}\")\n",
    "\n",
    "# Print the status\n",
    "print(\"✅ Final status complete:\", new_generated_analysis.status_complete)\n",
    "\n",
    "# Let's look at the updated script\n",
    "# Write R script to the current directory\n",
    "script_path = Path(sample_task.task_name.replace(\" \", \"_\") + \"_analysis.R\")\n",
    "script_path.write_text(new_generated_analysis.script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29702d1f",
   "metadata": {},
   "source": [
    "## Step 7: Iterative Agentic Capabilities\n",
    "Now that we've seen the power of feedback, let's put it into a function so that we can loop through the process multiple times. This will allow us to refine the LLM's output iteratively, improving the generated R code with each pass. We will also add some error handling to ensure that if something goes wrong, we can capture it and provide feedback to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "148c526d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Analysis generated!\n",
      "Completed Status: True\n",
      "\n",
      "LLM thoughts: The script loads the population pharmacokinetic data from a CSV file. It then defines a one-compartment model with inter-individual variability using nlmixr2 syntax. The model is fitted using the SAEM algorithm, and the results are printed along with basic goodness-of-fit plots.\n",
      "\n",
      "📝 Generated R script (922 characters):\n",
      "LLM response: completed successfully.\n"
     ]
    }
   ],
   "source": [
    "from llm_feedback import llm_feedback_loop\n",
    "\n",
    "results = llm_feedback_loop(\n",
    "    llm=llm_struct,\n",
    "    messages=new_messages,\n",
    "    task=sample_task,\n",
    "    max_iterations=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb57d6e",
   "metadata": {},
   "source": [
    "## What We Just Built: The Foundational Atom of an AI Agent\n",
    "\n",
    "🎉 Congratulations! You've successfully built and executed a workflow that represents the foundational atom of a true AI partner. By implementing the **Plan, Act, Observe, Reflect** cycle, we've solved several core challenges:\n",
    "\n",
    "1.  **Solved for Reliability:** We used Pydantic for structured outputs, moving beyond fragile scripts to the **auditable, reproducible workflows** required in our industry.\n",
    "2.  **Solved for Decision-Making:** We built an iterative feedback loop, creating a basic agent that can **interpret results and self-correct**, a leap beyond traditional automation.\n",
    "3.  **Solved for Security:** We used Ollama to run a powerful local LLM, ensuring all proprietary data and analysis **remain securely on-premises**.\n",
    "4.  **Solved for Expertise:** We showed how to provide data and code context, the first step towards encoding **deep domain knowledge** into our automated systems.\n",
    "\n",
    "This simple, powerful atom is the key building block for more advanced agents that can tackle increasingly complex tasks.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "With this foundation, we can now look ahead. Future explorations will build on this 'atom' to create more sophisticated capabilities:\n",
    "\n",
    "1. **Advanced Task Orchestration**: Composing our agentic atoms to tackle multi-step analyses.\n",
    "2. **RAG for Domain Knowledge**: Granting our agent access to a knowledge base of past analyses, allowing it to learn from experience.\n",
    "3. **Production Deployment**: Building in the validation, monitoring, and compliance checks needed for real-world use.\n",
    "\n",
    "## Try Your Own Tasks\n",
    "\n",
    "Modify the `task_details` in the notebook to experiment with:\n",
    "- Different types of PK/PD analysis\n",
    "- Population modeling with nlmixr2\n",
    "- Exposure-response analysis\n",
    "- Dose optimization scenarios\n",
    "\n",
    "The same framework scales from basic NCA to complex PopPK models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
